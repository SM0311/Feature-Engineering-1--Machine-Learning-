{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430641b4-d9b8-4ca0-9d38-32cd5f7f5aac",
   "metadata": {},
   "source": [
    "**Q1:** What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96978d-6a4f-4c8d-b2a1-722c61608296",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Answer -**\n",
    "Missing values in a dataset refer to the absence of a particular value or piece of information for one or more data points. These values are usually denoted as NA, NaN, or null values, depending on the software used.\n",
    "\n",
    "Handling missing values is essential because they can lead to biased and inaccurate results in data analysis, affecting the quality of the findings and conclusions drawn from the data. Missing values can also create issues when applying machine learning algorithms, as many of these models cannot handle missing data.\n",
    "\n",
    "However, some algorithms can handle missing values quite well. These algorithms include decision trees, k-Nearest Neighbors (k-NN), and Random Forests. These algorithms have built-in mechanisms to handle missing values by either ignoring the missing data or replacing them with imputed values based on certain rules or statistical methods.For example, decision trees can automatically bypass the missing values by branching off to a non-missing value, while k-NN can replace the missing values with the mean or median of the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b7988d-90f6-4901-9cdd-c49b9a08d119",
   "metadata": {},
   "source": [
    "**Q2:** List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58c7b1-15ec-444c-9be4-d45bcb954ea5",
   "metadata": {},
   "source": [
    "**Answer 2**\n",
    "\n",
    "There are several techniques that can be used to handle missing data in a dataset. Here are some examples with Python code:\n",
    "\n",
    "1. Deletion: This technique involves removing the rows or columns that contain missing values from the dataset. It can be applied when the missing values are sparse, and deleting them does not significantly affect the overall distribution of the data.\n",
    "\n",
    "Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05bd8cbc-7eb1-41e2-a373-89eb23081c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, np.nan, 8, np.nan, 10]})\n",
    "\n",
    "# drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "print(df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafce91-349a-47b9-9bc9-ea0329d31e8b",
   "metadata": {},
   "source": [
    "2. Imputation: This technique involves replacing the missing values with some estimates. It can be applied when the missing values are less than 5% of the data.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bea599-11d9-4dc5-829e-5d769886b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   8.0\n",
      "2  3.0   8.0\n",
      "3  4.0   8.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, np.nan, 8, np.nan, 10]})\n",
    "\n",
    "# create an imputer object\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit and transform the dataframe\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6c0cf-1638-4b98-b156-c1abba4e8ca9",
   "metadata": {},
   "source": [
    "3. Prediction: This technique involves using machine learning algorithms to predict the missing values. It can be applied when the missing values are random and the dataset is large.\n",
    "\n",
    "Example -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382e6fe-801a-45f0-822b-d7706d78173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, np.nan, 8, np.nan, 10]})\n",
    "\n",
    "# split the dataframe into training and testing sets\n",
    "train = df.dropna()\n",
    "test = df[df.isna().any(axis=1)]\n",
    "\n",
    "# create a linear regression object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lr.fit(train[['A']], train['B'])\n",
    "\n",
    "# predict the missing values\n",
    "test['B'] = lr.predict(test[['A']])\n",
    "\n",
    "# combine the training and testing sets\n",
    "df_predicted = pd.concat([train, test])\n",
    "\n",
    "print(df_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6999675-36b2-473c-9365-61f218e44d66",
   "metadata": {},
   "source": [
    "**Q3:** Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b8511-93ca-461f-bca0-1b9a2b0b5f97",
   "metadata": {},
   "source": [
    "**Answer-**\n",
    "\n",
    "Imbalanced data refers to a situation where the distribution of classes in a dataset is highly skewed, meaning that some classes have significantly fewer samples than others. For example, if you have a dataset with 1000 samples, and 900 of them belong to class A and only 100 belong to class B, then this dataset is highly imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased or inaccurate machine learning models. In such a situation, the model tends to favor the majority class and may perform poorly in predicting the minority class. For example, if we use the above dataset to train a binary classification model to predict class A or B, the model may perform very well in predicting class A but poorly in predicting class B.\n",
    "\n",
    "This can be problematic in many real-world scenarios, such as fraud detection, medical diagnosis, or rare disease detection, where the minority class is often more important and requires more attention.\n",
    "\n",
    "Therefore, it is essential to handle imbalanced data by using appropriate techniques such as resampling, adjusting class weights, or using different evaluation metrics that can account for the imbalanced nature of the data. By doing so, we can improve the performance of machine learning models and ensure that they are more robust and accurate in predicting all classes, including the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71e118-47b1-4ad3-a3f0-8a7fcce94b39",
   "metadata": {},
   "source": [
    "**Q4:** What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc5652-7275-418a-870a-e873ae230672",
   "metadata": {},
   "source": [
    "**Answer -** \n",
    "\n",
    "Up-sampling and down-sampling are two common techniques used to address imbalanced data by either increasing or decreasing the number of samples in each class. \n",
    "\n",
    "**Down-sampling** involves reducing the number of samples in the majority class, while up-sampling involves increasing the number of samples in the minority class.\n",
    "\n",
    "Here's an example scenario where up-sampling and down-sampling might be required:\n",
    "\n",
    "Suppose you have a dataset for credit card fraud detection with 100,000 transactions, out of which only 100 are fraudulent. In this case, the dataset is heavily imbalanced, and if we use it to train a machine learning model without handling the imbalance, the model might only learn to classify the majority class correctly and ignore the minority class.\n",
    "\n",
    "To handle this situation, we can use up-sampling to increase the number of fraudulent transactions in the dataset. We can do this by creating new synthetic samples by randomly selecting and slightly modifying existing fraudulent transactions. This will increase the number of fraudulent transactions in the dataset and balance the class distribution, making it easier for the model to learn.\n",
    "\n",
    "On the other hand, if the dataset had 100,000 fraudulent transactions and only 100 non-fraudulent transactions, we can use down-sampling to reduce the number of fraudulent transactions in the dataset. We can randomly select a subset of non-fraudulent transactions to keep in the dataset, thereby reducing the imbalance and making the dataset more suitable for training a machine learning model.\n",
    "\n",
    "In summary, up-sampling and down-sampling are two techniques that are commonly used to address imbalanced data. We can use them to increase or decrease the number of samples in each class, depending on the situation, to create a balanced dataset and improve the performance of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd8cf6-f087-47d4-b2c0-4d0252e6ca76",
   "metadata": {},
   "source": [
    "**Q5:** What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd427e3-1a2a-451c-9d31-1b0efa677f21",
   "metadata": {},
   "source": [
    "**Answer-** Data augmentation is a technique used in machine learning to generate new synthetic data from existing data. It is a popular technique to address the issue of limited data availability and improve the performance of machine learning models.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation technique that is commonly used to address the issue of imbalanced data. It works by creating new synthetic samples from the minority class in a way that maintains the shape and distribution of the minority class.\n",
    "\n",
    "The SMOTE algorithm works as follows:\n",
    "\n",
    "1. For each sample in the minority class, find its k nearest neighbors (typically k=5).\n",
    "2. Select one of the k neighbors at random.\n",
    "3. Generate a new sample by linearly interpolating between the selected sample and the original sample.\n",
    "4. Repeat steps 2-3 until the desired number of synthetic samples is generated.\n",
    "\n",
    "By creating new synthetic samples in this way, SMOTE can effectively balance the class distribution in the dataset and improve the performance of machine learning models on the minority class. Additionally, the use of SMOTE can help reduce overfitting and improve the generalizability of the model.\n",
    "\n",
    "However, it is important to note that SMOTE may not work well for all datasets, and it is always recommended to carefully evaluate the performance of the model after using SMOTE to ensure that it is not overfitting or introducing any other biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79cb06-636e-4913-a81e-936ff858cae0",
   "metadata": {},
   "source": [
    "**Q6:** What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf027ec4-3c7b-4489-a625-c8b2e70c283d",
   "metadata": {},
   "source": [
    "**Answer -** Outliers are data points that are significantly different from other data points in a dataset. They are often considered unusual, abnormal, or erroneous data points that do not fit with the rest of the data. Outliers can occur due to measurement errors, data processing errors, or other natural phenomena that cause extreme values.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on the statistical analysis and modeling of the data. Outliers can skew the distribution of the data, increase the variance, and reduce the accuracy of machine learning models. Outliers can also influence the parameter estimation and lead to biased results.\n",
    "\n",
    "For example, in linear regression, an outlier can have a significant impact on the slope of the line, leading to a poor fit and inaccurate predictions. In clustering, outliers can form their own cluster, which can affect the accuracy of the clustering algorithm.\n",
    "\n",
    "To handle outliers, there are several techniques that can be used, such as removing them, transforming the data, or treating them as missing values. However, it is important to carefully evaluate the impact of these techniques on the data and the modeling results to ensure that they do not introduce any other biases or distortions.\n",
    "\n",
    "In summary, handling outliers is essential to ensure that the data is accurately represented and to improve the performance of machine learning models. By identifying and handling outliers appropriately, we can ensure that the statistical analysis and modeling of the data are robust and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca43b4-cd9a-418d-a10b-5c9be8ee23d3",
   "metadata": {},
   "source": [
    "**Q7:** You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea3021-142b-44d3-a5bd-cb9ecaf7435b",
   "metadata": {},
   "source": [
    "**Answer -** There are several techniques that can be used to handle missing data in customer data analysis. Here are some common techniques:\n",
    "\n",
    "1. Deleting missing data: If the percentage of missing data is small and it does not significantly affect the analysis, we can simply delete the missing data points or entire rows or columns with missing data.\n",
    "\n",
    "2. Imputing missing data: We can estimate the missing data by imputing values based on the available data. Common imputation techniques include mean imputation, median imputation, mode imputation, and regression imputation.\n",
    "\n",
    "   For example, if a customer's age is missing, we can impute the missing value with the mean age of other customers.\n",
    "\n",
    "3. Hot-deck imputation: Hot-deck imputation is a technique where missing values are imputed based on values from similar data points. This can be done by selecting a random value from a similar customer or the closest neighbor.\n",
    "\n",
    "4. Multiple imputation: Multiple imputation involves creating multiple imputed datasets and analyzing each one separately. This technique is useful when the missing data is not completely at random and is dependent on other variables.\n",
    "\n",
    "   For example, if a customer's annual income is missing, it may depend on their education level and job title. In this case, multiple imputation can be used to impute the missing data based on other variables.\n",
    "\n",
    "5. Using algorithms that handle missing data: Some algorithms, such as decision trees and random forests, can handle missing data without any imputation or data modification.\n",
    "\n",
    "   For example, decision trees can split on missing data, and random forests can use other variables to predict the missing values.\n",
    "\n",
    "These are some techniques that can be used to handle missing data in customer data analysis. However, it is important to choose the appropriate technique based on the nature and extent of the missing data and carefully evaluate the impact of the missing data handling technique on the analysis results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b800aa-4e92-4915-9ecb-4c4a1ff369d4",
   "metadata": {},
   "source": [
    "**Q8:** You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aadbc7-d7f2-48ad-8476-1832ad124b08",
   "metadata": {},
   "source": [
    "**Answer-** To determine if the missing data is missing at random or if there is a pattern to the missing data, we can use several strategies, including:\n",
    "\n",
    "1. Visualizing missing data: We can create visualizations, such as heatmaps or histograms, to identify patterns in the missing data. This can help us identify if the missing data is missing at random or if it is related to specific variables or observations.\n",
    "\n",
    "2. Correlation analysis: We can calculate the correlation between the missing data and other variables in the dataset. If there is a high correlation between the missing data and other variables, it may indicate that the missing data is not missing at random.\n",
    "\n",
    "3. Hypothesis testing: We can conduct statistical tests, such as chi-square tests or t-tests, to determine if there is a significant difference between the missing data and other variables in the dataset. If there is a significant difference, it may indicate that the missing data is not missing at random.\n",
    "\n",
    "4. Pattern recognition: We can use machine learning algorithms to identify patterns in the missing data. For example, we can use clustering algorithms to group similar missing data points and identify patterns in the missing data.\n",
    "\n",
    "5. Domain knowledge: We can use our domain knowledge and expertise to identify if there is a pattern to the missing data. For example, if we are working with medical data and find that patient information is missing for a specific hospital, it may indicate that there is a problem with the data collection process at that hospital.\n",
    "\n",
    "These strategies can help us determine if the missing data is missing at random or if there is a pattern to the missing data. Once we have identified the pattern in the missing data, we can choose an appropriate missing data handling technique to handle the missing data and ensure that our analysis is accurate and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95508362-6527-4ec9-95d5-0cf38eb0d1e9",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569e99f-3daf-441f-9067-802372e8bc9a",
   "metadata": {},
   "source": [
    "**Answer -** When working with an imbalanced dataset, where the majority of patients do not have the condition of interest, there are several strategies we can use to evaluate the performance of a machine learning model.\n",
    "\n",
    "Here are some common strategies:\n",
    "\n",
    "1. **Confusion matrix:** The confusion matrix is a table that shows the true positive, true negative, false positive, and false negative values of the model's predictions. This can help us evaluate the performance of the model and identify any issues with false positives or false negatives.\n",
    "\n",
    "2. **Precision, recall, and F1 score:** Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positive instances. The F1 score is the harmonic mean of precision and recall. These metrics are particularly useful for imbalanced datasets, as they take into account both false positives and false negatives.\n",
    "\n",
    "3. **Receiver Operating Characteristic (ROC) curve:** The ROC curve plots the true positive rate against the false positive rate for different threshold values. This curve can help us evaluate the performance of the model at different thresholds and identify the optimal threshold value.\n",
    "\n",
    "4. **Area under the curve (AUC):** The AUC is the area under the ROC curve and provides a single value that summarizes the performance of the model across all possible threshold values. A high AUC indicates that the model is performing well, even on an imbalanced dataset.\n",
    "\n",
    "5. **Class weights:** Class weights can be used to adjust the weight of the minority class during training. This can help the model to better learn from the minority class and improve performance on the imbalanced dataset.\n",
    "\n",
    "6. **Resampling techniques:** Resampling techniques, such as oversampling the minority class or undersampling the majority class, can be used to balance the dataset and improve model performance.\n",
    "\n",
    "These are some strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset. It is important to carefully evaluate the results and choose an appropriate strategy based on the specific needs of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2880adf-7aa5-4440-a094-83bcf1224205",
   "metadata": {},
   "source": [
    "**Q10:** When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f512445-7143-4ff4-853d-d8dea652f1b5",
   "metadata": {},
   "source": [
    "**Answer -** When dealing with an imbalanced dataset in which the majority class represents the satisfied customers, one option to balance the dataset is to down-sample the majority class. \n",
    "\n",
    "Here are some methods that can be employed to achieve this:\n",
    "\n",
    "1. **Random under-sampling:** This method involves randomly selecting a subset of the majority class instances to match the number of instances in the minority class. The drawback of this method is that it may discard useful information from the majority class and result in a loss of accuracy.\n",
    "\n",
    "2. **Cluster-based under-sampling:** This method involves clustering the majority class instances and then selecting a representative sample from each cluster. This can help retain more information from the majority class and result in better performance.\n",
    "\n",
    "3. **Tomek links:** This method involves identifying pairs of instances from the minority and majority classes that are closest to each other and removing the majority class instance from each pair. This can help remove noisy instances from the majority class.\n",
    "\n",
    "4. **Edited nearest neighbors:** This method involves identifying instances from the majority class that are misclassified by the nearest neighbor classifier and removing them. This can help remove noisy instances from the majority class and improve performance.\n",
    "\n",
    "Here is an example of how to down-sample the majority class using random under-sampling in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861f009-932e-4138-ba97-fe5399fab161",
   "metadata": {},
   "source": [
    "          from sklearn.utils import resample\n",
    "\n",
    "         # Separate majority and minority classes\n",
    "         majority_class = df[df.satisfaction == 'satisfied']\n",
    "         minority_class = df[df.satisfaction == 'dissatisfied']\n",
    "\n",
    "        # Downsample majority class\n",
    "         downsampled_majority = resample(majority_class,\n",
    "                                replace=False,\n",
    "                                n_samples=len(minority_class),\n",
    "                                random_state=42)\n",
    "\n",
    "      # Combine minority class with downsampled majority class\n",
    "      balanced_df = pd.concat([downsampled_majority, minority_class])\n",
    "\n",
    "In this example, we first separate the majority and minority classes and then use the resample function from the sklearn.utils module to down-sample the majority class to match the number of instances in the minority class. We set replace=False to perform sampling without replacement, and random_state=42 to ensure reproducibility. Finally, we combine the minority and downsampled majority classes into a single balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66e87e-f544-456f-acc3-54dcce7b7398",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b8d7a-4897-4e97-9e3c-79af576723d0",
   "metadata": {},
   "source": [
    "**Answer---** \n",
    "\n",
    "When dealing with an imbalanced dataset in which the minority class represents a rare event, one option to balance the dataset is to up-sample the minority class. Here are some methods that can be employed to achieve this:\n",
    "\n",
    "1. **Random over-sampling:** This method involves randomly duplicating instances from the minority class to match the number of instances in the majority class. The drawback of this method is that it may result in overfitting and reduced generalization performance.\n",
    "\n",
    "2. **Synthetic minority oversampling technique (SMOTE):** This method involves creating synthetic instances of the minority class by interpolating between neighboring instances. This can help retain more information from the minority class and result in better performance.\n",
    "\n",
    "3. **Adaptive synthetic sampling (ADASYN):** This method is similar to SMOTE but involves creating more synthetic instances for difficult-to-learn minority class instances, resulting in a more balanced dataset.\n",
    "\n",
    "Here is an example of how to up-sample the minority class using SMOTE in Python:\n",
    "\n",
    "\n",
    "          from imblearn.over_sampling import SMOTE\n",
    "\n",
    "           # Separate majority and minority classes\n",
    "             majority_class = df[df.occurrence == 0]\n",
    "             minority_class = df[df.occurrence == 1]\n",
    "\n",
    "            # Upsample minority class using SMOTE\n",
    "              smote = SMOTE(random_state=42)\n",
    "              X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "           # Combine majority class with upsampled minority class\n",
    "    balanced_df = pd.concat([majority_class, X_resampled, y_resampled], axis=1)\n",
    "\n",
    "\n",
    "In this example, we first separate the majority and minority classes and then use the SMOTE function from the imblearn.over_sampling module to up-sample the minority class using synthetic instances. We set random_state=42 to ensure reproducibility. Finally, we combine the majority and up-sampled minority classes into a single balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67082c3-2398-49e7-8605-bc8a49aff582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
